{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_teacher(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_main():\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=1000, shuffle=True)\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    teacher_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_teacher(model, device, train_loader, optimizer, epoch)\n",
    "        loss, acc = test_teacher(model, device, test_loader)\n",
    "        \n",
    "        teacher_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"teacherres.pt\")\n",
    "    return model, teacher_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.2233, accuracy: 5776/10000 (58%)\n",
      "Train epoch 2: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.7882, accuracy: 7294/10000 (73%)\n",
      "Train epoch 3: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6706, accuracy: 7751/10000 (78%)\n",
      "Train epoch 4: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5938, accuracy: 8003/10000 (80%)\n",
      "Train epoch 5: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5254, accuracy: 8244/10000 (82%)\n",
      "Train epoch 6: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6567, accuracy: 7970/10000 (80%)\n",
      "Train epoch 7: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6389, accuracy: 8049/10000 (80%)\n",
      "Train epoch 8: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4362, accuracy: 8521/10000 (85%)\n",
      "Train epoch 9: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4344, accuracy: 8569/10000 (86%)\n",
      "Train epoch 10: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5235, accuracy: 8312/10000 (83%)\n",
      "Train epoch 11: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4192, accuracy: 8610/10000 (86%)\n",
      "Train epoch 12: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4337, accuracy: 8640/10000 (86%)\n",
      "Train epoch 13: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3996, accuracy: 8723/10000 (87%)\n",
      "Train epoch 14: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3976, accuracy: 8728/10000 (87%)\n",
      "Train epoch 15: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4607, accuracy: 8612/10000 (86%)\n",
      "Train epoch 16: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3886, accuracy: 8799/10000 (88%)\n",
      "Train epoch 17: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4053, accuracy: 8730/10000 (87%)\n",
      "Train epoch 18: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3940, accuracy: 8804/10000 (88%)\n",
      "Train epoch 19: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4065, accuracy: 8751/10000 (88%)\n",
      "Train epoch 20: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3701, accuracy: 8870/10000 (89%)\n",
      "Train epoch 21: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3507, accuracy: 8963/10000 (90%)\n",
      "Train epoch 22: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3310, accuracy: 8988/10000 (90%)\n",
      "Train epoch 23: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4358, accuracy: 8705/10000 (87%)\n",
      "Train epoch 24: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3525, accuracy: 8948/10000 (89%)\n",
      "Train epoch 25: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3513, accuracy: 8939/10000 (89%)\n",
      "Train epoch 26: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3393, accuracy: 8996/10000 (90%)\n",
      "Train epoch 27: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3643, accuracy: 8938/10000 (89%)\n",
      "Train epoch 28: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3525, accuracy: 8997/10000 (90%)\n",
      "Train epoch 29: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3669, accuracy: 8932/10000 (89%)\n",
      "Train epoch 30: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3346, accuracy: 9042/10000 (90%)\n",
      "Train epoch 31: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3221, accuracy: 9067/10000 (91%)\n",
      "Train epoch 32: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4080, accuracy: 8863/10000 (89%)\n",
      "Train epoch 33: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3600, accuracy: 8929/10000 (89%)\n",
      "Train epoch 34: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3234, accuracy: 9057/10000 (91%)\n",
      "Train epoch 35: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3158, accuracy: 9104/10000 (91%)\n",
      "Train epoch 36: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3228, accuracy: 9049/10000 (90%)\n",
      "Train epoch 37: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.2802, accuracy: 9183/10000 (92%)\n",
      "Train epoch 38: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3282, accuracy: 9098/10000 (91%)\n",
      "Train epoch 39: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3985, accuracy: 8918/10000 (89%)\n",
      "Train epoch 40: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3079, accuracy: 9106/10000 (91%)\n",
      "Train epoch 41: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3537, accuracy: 9004/10000 (90%)\n",
      "Train epoch 42: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3253, accuracy: 9077/10000 (91%)\n",
      "Train epoch 43: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3318, accuracy: 9074/10000 (91%)\n",
      "Train epoch 44: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3527, accuracy: 9036/10000 (90%)\n",
      "Train epoch 45: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3551, accuracy: 9025/10000 (90%)\n",
      "Train epoch 46: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3205, accuracy: 9097/10000 (91%)\n",
      "Train epoch 47: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3187, accuracy: 9098/10000 (91%)\n",
      "Train epoch 48: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3092, accuracy: 9149/10000 (91%)\n",
      "Train epoch 49: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.3827, accuracy: 8994/10000 (90%)\n",
      "Train epoch 50: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.2948, accuracy: 9168/10000 (92%)\n"
     ]
    }
   ],
   "source": [
    "teacher_model, teacher_history = teacher_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "#         self.model = nn.Sequential(self.conv1, self.bn1, self.layers, self.linear)\n",
    "#         self._initialize_weights()\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation(y, labels, teacher_scores, temp, alpha):\n",
    "    return nn.KLDivLoss()(F.log_softmax(y / temp, dim=1), F.softmax(teacher_scores / temp, dim=1)) * (\n",
    "            temp * temp * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (left): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacherNet = ResNet18()\n",
    "teacherNet.load_state_dict(torch.load(\"./teacherres.pt\"))\n",
    "teacherNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacherNet.train(mode=False)\n",
    "teacherNet = teacherNet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomGrayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform1)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=50,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_kd(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        teacher_output = teacherNet(data)\n",
    "#         teacher_output = teacherNet.detach()  # 切断老师网络的反向传播，感谢B站“淡淡的落”的提醒\n",
    "        loss = distillation(output, target, teacher_output, temp=5.0, alpha=0.7)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')\n",
    "\n",
    "def test_student_kd(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_kd_main():\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=1000, shuffle=True)\n",
    "\n",
    "#     model = StudentNet().to(device)\n",
    "    studentNet = MobileNet()\n",
    "    studentNet.load_state_dict(torch.load(\"./student.pt\"))\n",
    "    studentNet.eval()\n",
    "#     studentNet.train(mode=False)\n",
    "    model = studentNet.to(device)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters())\n",
    "    \n",
    "    student_history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_student_kd(model, device, train_loader, optimizer, epoch)\n",
    "        loss, acc = test_student_kd(model, device, test_loader)\n",
    "        student_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"student_kdrss.pt\")\n",
    "    return model, student_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6321, accuracy: 8214/10000 (82%)\n",
      "Train epoch 2: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6231, accuracy: 8273/10000 (83%)\n",
      "Train epoch 3: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5996, accuracy: 8326/10000 (83%)\n",
      "Train epoch 4: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6182, accuracy: 8338/10000 (83%)\n",
      "Train epoch 5: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6213, accuracy: 8323/10000 (83%)\n",
      "Train epoch 6: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6394, accuracy: 8340/10000 (83%)\n",
      "Train epoch 7: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6316, accuracy: 8324/10000 (83%)\n",
      "Train epoch 8: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6140, accuracy: 8398/10000 (84%)\n",
      "Train epoch 9: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6257, accuracy: 8354/10000 (84%)\n",
      "Train epoch 10: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6125, accuracy: 8385/10000 (84%)\n"
     ]
    }
   ],
   "source": [
    "student_kd_model, student_kd_history = student_kd_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
