{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256        # 批的大小\n",
    "learning_rate = 1e-3    # 学习率\n",
    "num_epoches = 10       # 遍历训练集的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(3,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            #2\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #3\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            #4\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #5\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #6\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #7\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #8\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #9\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #10\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #11\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #12\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #13\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.AvgPool2d(kernel_size=1,stride=1),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            #14\n",
    "            nn.Linear(512,4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #15\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #16\n",
    "            nn.Linear(4096,num_classes),\n",
    "            )\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        #        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        #        print(out.shape)\n",
    "        out = self.classifier(out)\n",
    "        #        print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "'''定义loss和optimizer'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* epoch 1 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:41,  1.34it/s]\n",
      "<ipython-input-13-35f34aa6cf4c>:35: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  img = Variable(img, volatile=True).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1 epoch, Loss: 2.026781, Acc: 0.230200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-35f34aa6cf4c>:36: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  label = Variable(label, volatile=True).cuda()\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.668141, Acc: 0.370300\n",
      "\n",
      "************************* epoch 2 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [10:33,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 2 epoch, Loss: 1.563775, Acc: 0.424680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.693176, Acc: 0.388300\n",
      "\n",
      "************************* epoch 3 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [10:08,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 3 epoch, Loss: 1.306501, Acc: 0.523720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.411429, Acc: 0.499800\n",
      "\n",
      "************************* epoch 4 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [10:29,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 4 epoch, Loss: 1.147672, Acc: 0.585320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.192472, Acc: 0.570300\n",
      "\n",
      "************************* epoch 5 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:53,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 5 epoch, Loss: 1.016525, Acc: 0.637060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.514249, Acc: 0.523400\n",
      "\n",
      "************************* epoch 6 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:53,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 6 epoch, Loss: 0.896041, Acc: 0.681060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.060386, Acc: 0.626000\n",
      "\n",
      "************************* epoch 7 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:54,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 7 epoch, Loss: 0.788433, Acc: 0.720340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.135527, Acc: 0.618100\n",
      "\n",
      "************************* epoch 8 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:53,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 8 epoch, Loss: 0.694712, Acc: 0.753720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.312493, Acc: 0.443200\n",
      "\n",
      "************************* epoch 9 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:53,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 9 epoch, Loss: 0.605033, Acc: 0.789200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.005858, Acc: 0.665900\n",
      "\n",
      "************************* epoch 10 *************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [09:54,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 10 epoch, Loss: 0.512608, Acc: 0.821800\n",
      "Test Loss: 1.403495, Acc: 0.595200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    print('*' * 25, 'epoch {}'.format(epoch + 1), '*' * 25)  # .format为输出格式，formet括号里的即为左边花括号的输出\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader, 1)):\n",
    "\n",
    "        img, label = data\n",
    "        # cuda\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        img = Variable(img)\n",
    "        label = Variable(label)\n",
    "        # 向前传播\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.item() * label.size(0)\n",
    "        _, pred = torch.max(out, 1)  # 预测最大值所在的位置标签\n",
    "        num_correct = (pred == label).sum()\n",
    "        accuracy = (pred == label).float().mean()\n",
    "        running_acc += num_correct.item()\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(train_dataset))))\n",
    "\n",
    "    model.eval()  # 模型评估\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for data in test_loader:  # 测试模型\n",
    "        img, label = data\n",
    "        if use_gpu:\n",
    "            img = Variable(img, volatile=True).cuda()\n",
    "            label = Variable(label, volatile=True).cuda()\n",
    "        else:\n",
    "            img = Variable(img, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.item() * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        eval_acc += num_correct.item()\n",
    "    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        test_dataset)), eval_acc / (len(test_dataset))))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10('./cifar10_data/',train=True,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 500, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "#         self.model = nn.Sequential(self.conv1, self.bn1, self.layers, self.linear)\n",
    "#         self._initialize_weights()\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=5\n",
    "net=MobileNet()\n",
    "cost=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2323, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.022\n",
      "tensor(2.2562, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.045\n",
      "tensor(2.2259, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.067\n",
      "tensor(2.2152, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.089\n",
      "tensor(2.2356, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.112\n",
      "tensor(2.1922, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.134\n",
      "tensor(2.2051, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.156\n",
      "tensor(2.2179, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.178\n",
      "tensor(2.1495, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.199\n",
      "tensor(2.1512, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.221\n",
      "tensor(2.1843, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.243\n",
      "tensor(2.1182, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.264\n",
      "tensor(2.0814, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.285\n",
      "tensor(2.0294, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.305\n",
      "tensor(2.0764, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.326\n",
      "tensor(2.0531, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.346\n",
      "tensor(1.9775, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.366\n",
      "tensor(2.0242, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.386\n",
      "tensor(2.0777, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.407\n",
      "tensor(1.9901, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.427\n",
      "tensor(2.0229, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.447\n",
      "tensor(2.0198, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.467\n",
      "tensor(2.0108, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.487\n",
      "tensor(1.9398, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.507\n",
      "tensor(2.0246, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.527\n",
      "tensor(2.0337, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.547\n",
      "tensor(1.9825, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.567\n",
      "tensor(1.9284, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.587\n",
      "tensor(1.9212, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.606\n",
      "tensor(1.9518, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.625\n",
      "tensor(1.9130, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.644\n",
      "tensor(1.9884, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.664\n",
      "tensor(1.9154, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.683\n",
      "tensor(1.9511, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.703\n",
      "tensor(1.9462, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.722\n",
      "tensor(1.8747, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.741\n",
      "tensor(1.8481, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.760\n",
      "tensor(1.8731, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.778\n",
      "tensor(1.8861, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.797\n",
      "tensor(1.8898, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.816\n",
      "tensor(1.9044, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.835\n",
      "tensor(1.8816, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.854\n",
      "tensor(1.8374, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.872\n",
      "tensor(1.8404, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.891\n",
      "tensor(1.7912, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.909\n",
      "tensor(1.8907, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.928\n",
      "tensor(1.7966, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.946\n",
      "tensor(1.8774, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.964\n",
      "tensor(1.8730, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:0.983\n",
      "tensor(1.8714, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.002\n",
      "tensor(1.8165, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.020\n",
      "tensor(1.8043, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.038\n",
      "tensor(1.8167, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.056\n",
      "tensor(1.7986, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.074\n",
      "tensor(1.7770, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.092\n",
      "tensor(1.8758, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.111\n",
      "tensor(1.7700, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.128\n",
      "tensor(1.8046, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.146\n",
      "tensor(1.7980, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.164\n",
      "tensor(1.8373, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.183\n",
      "tensor(1.7748, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.201\n",
      "tensor(1.7497, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.218\n",
      "tensor(1.7828, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.236\n",
      "tensor(1.8180, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.254\n",
      "tensor(1.8108, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.272\n",
      "tensor(1.7015, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.289\n",
      "tensor(1.7561, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.307\n",
      "tensor(1.8235, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.325\n",
      "tensor(1.7679, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.343\n",
      "tensor(1.7961, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.361\n",
      "tensor(1.8244, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.379\n",
      "tensor(1.7808, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.397\n",
      "tensor(1.7784, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.414\n",
      "tensor(1.8184, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.433\n",
      "tensor(1.7367, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.450\n",
      "tensor(1.6648, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.467\n",
      "tensor(1.7400, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.484\n",
      "tensor(1.7503, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.502\n",
      "tensor(1.7188, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.519\n",
      "tensor(1.6591, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.535\n",
      "tensor(1.7032, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.552\n",
      "tensor(1.7829, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.570\n",
      "tensor(1.6798, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.587\n",
      "tensor(1.6806, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.604\n",
      "tensor(1.6946, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.621\n",
      "tensor(1.7423, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.638\n",
      "tensor(1.6875, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.655\n",
      "tensor(1.6972, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.672\n",
      "tensor(1.7039, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.689\n",
      "tensor(1.6997, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.706\n",
      "tensor(1.7460, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.723\n",
      "tensor(1.6736, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.740\n",
      "tensor(1.6739, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.757\n",
      "tensor(1.7255, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.774\n",
      "tensor(1.7190, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.791\n",
      "tensor(1.7127, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.809\n",
      "tensor(1.6578, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.825\n",
      "tensor(1.6605, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.842\n",
      "tensor(1.6403, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.858\n",
      "tensor(1.6083, grad_fn=<NllLossBackward>)\n",
      "[1,0] loss:1.874\n",
      "tensor(1.6449, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.016\n",
      "tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.032\n",
      "tensor(1.5493, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.047\n",
      "tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.063\n",
      "tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.077\n",
      "tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.092\n",
      "tensor(1.5857, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.108\n",
      "tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.123\n",
      "tensor(1.5428, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.139\n",
      "tensor(1.3845, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.153\n",
      "tensor(1.5421, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.168\n",
      "tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.183\n",
      "tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.199\n",
      "tensor(1.5407, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.214\n",
      "tensor(1.5151, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.229\n",
      "tensor(1.4609, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.244\n",
      "tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.259\n",
      "tensor(1.4079, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.273\n",
      "tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.289\n",
      "tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.303\n",
      "tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.319\n",
      "tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.334\n",
      "tensor(1.5100, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.349\n",
      "tensor(1.5755, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.365\n",
      "tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.380\n",
      "tensor(1.4372, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.395\n",
      "tensor(1.4014, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.409\n",
      "tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.423\n",
      "tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.439\n",
      "tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.454\n",
      "tensor(1.4689, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.469\n",
      "tensor(1.5981, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.485\n",
      "tensor(1.6118, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.501\n",
      "tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.516\n",
      "tensor(1.5201, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.531\n",
      "tensor(1.4616, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.546\n",
      "tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.561\n",
      "tensor(1.5893, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.577\n",
      "tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.608\n",
      "tensor(1.5232, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.623\n",
      "tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.638\n",
      "tensor(1.4950, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.653\n",
      "tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.668\n",
      "tensor(1.5626, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.684\n",
      "tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.699\n",
      "tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.714\n",
      "tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.729\n",
      "tensor(1.5535, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.744\n",
      "tensor(1.6130, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.760\n",
      "tensor(1.4699, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.775\n",
      "tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.791\n",
      "tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.806\n",
      "tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.821\n",
      "tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.836\n",
      "tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.851\n",
      "tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.866\n",
      "tensor(1.5835, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.882\n",
      "tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.897\n",
      "tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.912\n",
      "tensor(1.6326, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.928\n",
      "tensor(1.6235, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.945\n",
      "tensor(1.4577, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.959\n",
      "tensor(1.5473, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.975\n",
      "tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:0.990\n",
      "tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.006\n",
      "tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.021\n",
      "tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.036\n",
      "tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.051\n",
      "tensor(1.6121, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.067\n",
      "tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.083\n",
      "tensor(1.4652, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.097\n",
      "tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.112\n",
      "tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.127\n",
      "tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.142\n",
      "tensor(1.5427, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.158\n",
      "tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.173\n",
      "tensor(1.4316, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.187\n",
      "tensor(1.5337, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.203\n",
      "tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.218\n",
      "tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.233\n",
      "tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.248\n",
      "tensor(1.4357, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.262\n",
      "tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.278\n",
      "tensor(1.4487, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.292\n",
      "tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.307\n",
      "\n",
      "tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.322\n",
      "tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.337\n",
      "tensor(1.5781, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.353\n",
      "tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.368\n",
      "tensor(1.4669, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.383\n",
      "tensor(1.5570, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.399\n",
      "tensor(1.5533, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.414\n",
      "tensor(1.4559, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.429\n",
      "tensor(1.4527, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.443\n",
      "tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.458\n",
      "tensor(1.4648, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.473\n",
      "tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.488\n",
      "tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.503\n",
      "tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "[2,1] loss:1.518\n",
      "tensor(1.3488, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.013\n",
      "tensor(1.1984, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.025\n",
      "tensor(1.2733, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.038\n",
      "tensor(1.2778, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.051\n",
      "tensor(1.2648, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.064\n",
      "tensor(1.2020, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.076\n",
      "tensor(1.1901, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.088\n",
      "tensor(1.2692, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.100\n",
      "tensor(1.2637, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.113\n",
      "tensor(1.2733, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.126\n",
      "tensor(1.2830, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.138\n",
      "tensor(1.2298, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.151\n",
      "tensor(1.2041, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.163\n",
      "tensor(1.1766, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.175\n",
      "tensor(1.1581, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.186\n",
      "tensor(1.2613, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.199\n",
      "tensor(1.3436, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.212\n",
      "tensor(1.2175, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.224\n",
      "tensor(1.3037, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.237\n",
      "tensor(1.2593, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.250\n",
      "tensor(1.2922, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.263\n",
      "tensor(1.3309, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.276\n",
      "tensor(1.2479, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.289\n",
      "tensor(1.2971, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.302\n",
      "tensor(1.2215, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.314\n",
      "tensor(1.2138, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.326\n",
      "tensor(1.2107, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.338\n",
      "tensor(1.3455, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.352\n",
      "tensor(1.3241, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.365\n",
      "tensor(1.3483, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.378\n",
      "tensor(1.2383, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.391\n",
      "tensor(1.2265, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.403\n",
      "tensor(1.3016, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.416\n",
      "tensor(1.3010, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.429\n",
      "tensor(1.2731, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.442\n",
      "tensor(1.1405, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.453\n",
      "tensor(1.3044, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.466\n",
      "tensor(1.3544, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.480\n",
      "tensor(1.1901, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.492\n",
      "tensor(1.3594, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.505\n",
      "tensor(1.2875, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.518\n",
      "tensor(1.2588, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.531\n",
      "tensor(1.3522, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.544\n",
      "tensor(1.3518, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.558\n",
      "tensor(1.3048, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.571\n",
      "tensor(1.2667, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.583\n",
      "tensor(1.3388, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.597\n",
      "tensor(1.3380, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.610\n",
      "tensor(1.2953, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.623\n",
      "tensor(1.3289, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.636\n",
      "tensor(1.3590, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.650\n",
      "tensor(1.2893, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.663\n",
      "tensor(1.2398, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.675\n",
      "tensor(1.2696, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.688\n",
      "tensor(1.3608, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.702\n",
      "tensor(1.2893, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.715\n",
      "tensor(1.3823, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.728\n",
      "tensor(1.3774, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.742\n",
      "tensor(1.3426, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.756\n",
      "tensor(1.3632, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.769\n",
      "tensor(1.3853, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.783\n",
      "tensor(1.3558, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.797\n",
      "tensor(1.3759, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.810\n",
      "tensor(1.3868, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.824\n",
      "tensor(1.3304, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.837\n",
      "tensor(1.3001, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.850\n",
      "tensor(1.3454, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.864\n",
      "tensor(1.3169, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.877\n",
      "tensor(1.3533, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.891\n",
      "tensor(1.3370, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.904\n",
      "tensor(1.2699, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.917\n",
      "tensor(1.3008, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.930\n",
      "tensor(1.3134, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.943\n",
      "tensor(1.3070, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.956\n",
      "tensor(1.3634, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.970\n",
      "tensor(1.3135, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.983\n",
      "tensor(1.3352, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:0.996\n",
      "tensor(1.3025, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4092, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.023\n",
      "tensor(1.2454, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.036\n",
      "tensor(1.3402, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.049\n",
      "tensor(1.3616, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.063\n",
      "tensor(1.3166, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.076\n",
      "tensor(1.3154, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.089\n",
      "tensor(1.3211, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.102\n",
      "tensor(1.3668, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.116\n",
      "tensor(1.3220, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.129\n",
      "tensor(1.3131, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.142\n",
      "tensor(1.3128, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.155\n",
      "tensor(1.3595, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.169\n",
      "tensor(1.2586, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.182\n",
      "tensor(1.2826, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.194\n",
      "tensor(1.4015, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.208\n",
      "tensor(1.3405, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.222\n",
      "tensor(1.2437, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.234\n",
      "tensor(1.2746, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.247\n",
      "tensor(1.3569, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.260\n",
      "tensor(1.3363, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.274\n",
      "tensor(1.3426, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.287\n",
      "tensor(1.3466, grad_fn=<NllLossBackward>)\n",
      "[3,2] loss:1.301\n",
      "tensor(1.1051, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.011\n",
      "tensor(1.0389, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.021\n",
      "tensor(1.0388, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.032\n",
      "tensor(1.1800, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.044\n",
      "tensor(1.0470, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.054\n",
      "tensor(1.0628, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.065\n",
      "tensor(1.1390, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.076\n",
      "tensor(1.0775, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.087\n",
      "tensor(0.9088, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.096\n",
      "tensor(1.0369, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.106\n",
      "tensor(1.0591, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.117\n",
      "tensor(1.0137, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.127\n",
      "tensor(1.0945, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.138\n",
      "tensor(0.9667, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.148\n",
      "tensor(1.0248, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.158\n",
      "tensor(0.9743, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.168\n",
      "tensor(1.0371, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.178\n",
      "tensor(0.9733, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.188\n",
      "tensor(1.0750, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.199\n",
      "tensor(1.0238, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.209\n",
      "tensor(1.0122, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.219\n",
      "tensor(1.0217, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.229\n",
      "tensor(0.9437, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.239\n",
      "tensor(1.0018, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.249\n",
      "tensor(1.0584, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.259\n",
      "tensor(1.0187, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.269\n",
      "tensor(0.9992, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.279\n",
      "tensor(1.1259, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.291\n",
      "tensor(1.0453, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.301\n",
      "tensor(1.0659, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.312\n",
      "tensor(1.0527, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.322\n",
      "tensor(1.0506, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.333\n",
      "tensor(1.1086, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.344\n",
      "tensor(1.2181, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.356\n",
      "tensor(1.0402, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.366\n",
      "tensor(1.1409, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.378\n",
      "tensor(1.0606, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.388\n",
      "tensor(1.1699, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.400\n",
      "tensor(1.1359, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.411\n",
      "tensor(1.1399, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.423\n",
      "tensor(1.0164, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.433\n",
      "tensor(1.0804, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.444\n",
      "tensor(1.1956, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.456\n",
      "tensor(1.1901, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.468\n",
      "tensor(1.1704, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.479\n",
      "tensor(1.1273, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.491\n",
      "tensor(1.0555, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.501\n",
      "tensor(1.1816, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.513\n",
      "tensor(1.0879, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.524\n",
      "tensor(1.1275, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.535\n",
      "tensor(1.1532, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.547\n",
      "tensor(1.1209, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.558\n",
      "tensor(1.1291, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.569\n",
      "tensor(1.0878, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.580\n",
      "tensor(1.0091, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.590\n",
      "tensor(1.1515, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.602\n",
      "tensor(1.2480, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.614\n",
      "tensor(1.1693, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.626\n",
      "tensor(1.1746, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.638\n",
      "tensor(1.1501, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.649\n",
      "tensor(1.1082, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.660\n",
      "tensor(1.1404, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.672\n",
      "tensor(1.2122, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.684\n",
      "tensor(1.1248, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.695\n",
      "tensor(1.1162, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.706\n",
      "tensor(1.0990, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.717\n",
      "tensor(1.1685, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.729\n",
      "tensor(1.1428, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.740\n",
      "tensor(1.1299, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.752\n",
      "tensor(1.1479, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.763\n",
      "tensor(1.1463, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.775\n",
      "tensor(1.2371, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.787\n",
      "tensor(1.2468, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.799\n",
      "tensor(1.2704, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.812\n",
      "tensor(1.2027, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.824\n",
      "tensor(1.2049, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.836\n",
      "tensor(1.1718, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.848\n",
      "tensor(1.1557, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.859\n",
      "tensor(1.2437, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.872\n",
      "tensor(1.1545, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.883\n",
      "tensor(1.2324, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.896\n",
      "tensor(1.2026, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.908\n",
      "tensor(1.1842, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.920\n",
      "tensor(1.2482, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.932\n",
      "tensor(1.1812, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.944\n",
      "tensor(1.3041, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.957\n",
      "tensor(1.1058, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.968\n",
      "tensor(1.2063, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.980\n",
      "tensor(1.1468, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:0.991\n",
      "tensor(1.2006, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.004\n",
      "tensor(1.1780, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.015\n",
      "tensor(1.2180, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.027\n",
      "tensor(1.2190, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.040\n",
      "tensor(1.2519, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.052\n",
      "tensor(1.2411, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.065\n",
      "tensor(1.1444, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.076\n",
      "tensor(1.2327, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.088\n",
      "tensor(1.2680, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.101\n",
      "tensor(1.1334, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.112\n",
      "tensor(1.1434, grad_fn=<NllLossBackward>)\n",
      "[4,3] loss:1.124\n",
      "tensor(0.9779, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.010\n",
      "tensor(0.8857, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.019\n",
      "tensor(0.8954, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.028\n",
      "tensor(0.9578, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.037\n",
      "tensor(0.8995, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.046\n",
      "tensor(1.0006, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.056\n",
      "tensor(0.8826, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.065\n",
      "tensor(0.9398, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.074\n",
      "tensor(0.8002, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.082\n",
      "tensor(0.9175, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.092\n",
      "tensor(0.8704, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.100\n",
      "tensor(0.9290, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.110\n",
      "tensor(0.8155, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.118\n",
      "tensor(0.8979, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.127\n",
      "tensor(0.8463, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.135\n",
      "tensor(0.8505, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.144\n",
      "tensor(0.8704, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8479, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.161\n",
      "tensor(0.8603, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.169\n",
      "tensor(0.8769, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.178\n",
      "tensor(0.8301, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.187\n",
      "tensor(0.8369, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.195\n",
      "tensor(0.8551, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.203\n",
      "tensor(0.8931, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.212\n",
      "tensor(0.8157, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.221\n",
      "tensor(0.8259, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.229\n",
      "tensor(0.9069, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.238\n",
      "tensor(0.8658, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.247\n",
      "tensor(0.8193, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.255\n",
      "tensor(0.9229, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.264\n",
      "tensor(0.9003, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.273\n",
      "tensor(0.8297, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.281\n",
      "tensor(0.8719, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.290\n",
      "tensor(0.9715, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.300\n",
      "tensor(0.8950, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.309\n",
      "tensor(0.8496, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.317\n",
      "tensor(0.8838, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.326\n",
      "tensor(0.8939, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.335\n",
      "tensor(0.8847, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.344\n",
      "tensor(0.9942, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.354\n",
      "tensor(1.0099, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.364\n",
      "tensor(1.0125, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.374\n",
      "tensor(0.9024, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.383\n",
      "tensor(0.9552, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.392\n",
      "tensor(1.0023, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.403\n",
      "tensor(0.9510, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.412\n",
      "tensor(0.9682, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.422\n",
      "tensor(0.9861, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.432\n",
      "tensor(0.9276, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.441\n",
      "tensor(0.8980, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.450\n",
      "tensor(1.0296, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.460\n",
      "tensor(0.9633, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.470\n",
      "tensor(1.0098, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.480\n",
      "tensor(1.0226, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.490\n",
      "tensor(0.9725, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.500\n",
      "tensor(1.0647, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.510\n",
      "tensor(1.0024, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.520\n",
      "tensor(0.9479, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.530\n",
      "tensor(0.9951, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.540\n",
      "tensor(1.0151, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.550\n",
      "tensor(1.0136, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.560\n",
      "tensor(1.0712, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.571\n",
      "tensor(0.9158, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.580\n",
      "tensor(1.0364, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.590\n",
      "tensor(0.9948, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.600\n",
      "tensor(1.0516, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.611\n",
      "tensor(1.0207, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.621\n",
      "tensor(1.0462, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.632\n",
      "tensor(0.9665, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.641\n",
      "tensor(1.0364, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.652\n",
      "tensor(1.0181, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.662\n",
      "tensor(1.0734, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.672\n",
      "tensor(1.0215, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.683\n",
      "tensor(1.0481, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.693\n",
      "tensor(1.0681, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.704\n",
      "tensor(1.0116, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.714\n",
      "tensor(1.1181, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.725\n",
      "tensor(1.0675, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.736\n",
      "tensor(1.0946, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.747\n",
      "tensor(1.0393, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.757\n",
      "tensor(1.0412, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.768\n",
      "tensor(1.0883, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.778\n",
      "tensor(1.0424, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.789\n",
      "tensor(1.0920, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.800\n",
      "tensor(0.9499, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.809\n",
      "tensor(1.0582, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.820\n",
      "tensor(1.0929, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.831\n",
      "tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.842\n",
      "tensor(0.9972, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.852\n",
      "tensor(1.1141, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.863\n",
      "tensor(1.0899, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.874\n",
      "tensor(1.0006, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.884\n",
      "tensor(1.0821, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.895\n",
      "tensor(1.0550, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.905\n",
      "tensor(0.9326, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.915\n",
      "tensor(1.0549, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.925\n",
      "tensor(1.1181, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.936\n",
      "tensor(1.0389, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.947\n",
      "tensor(1.0781, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.957\n",
      "tensor(1.0546, grad_fn=<NllLossBackward>)\n",
      "[5,4] loss:0.968\n",
      "correct:65.282%\n"
     ]
    }
   ],
   "source": [
    "for k in range(epoch):\n",
    "    sum_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for i,data in enumerate(train_loader,0):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        \n",
    "        loss = cost(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss)\n",
    "        _,id = torch.max(outputs.data,1)\n",
    "        sum_loss+=loss.data\n",
    "        train_correct += torch.sum(id == labels.data)\n",
    "        print('[%d,%d] loss:%.03f'%(k+1,k,sum_loss/len(train_loader)))\n",
    "\n",
    "print('correct:%.03f%%'%(100*train_correct/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "#         progress = math.ceil(i / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        for i, data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            test_loss += F.cross_entropy(outputs, labels, reduction='sum').item()  # sum up batch loss\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomGrayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform1)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=50,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "#         progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_main():\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=1000, shuffle=True)\n",
    "\n",
    "    model = MobileNet().to(device)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters())\n",
    "    \n",
    "    student_history = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_student(model, device, train_loader, optimizer, epoch)\n",
    "        loss, acc = test_student(model, device, test_loader)\n",
    "        student_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"student.pt\")\n",
    "    return model, student_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 50000/50000\n",
      "Test: average loss: 1.3215, accuracy: 5365/10000 (54%)\n",
      "Train epoch 2: 50000/50000\n",
      "Test: average loss: 0.9672, accuracy: 6680/10000 (67%)\n",
      "Train epoch 3: 50000/50000\n",
      "Test: average loss: 0.7808, accuracy: 7272/10000 (73%)\n",
      "Train epoch 4: 50000/50000\n",
      "Test: average loss: 0.7796, accuracy: 7416/10000 (74%)\n",
      "Train epoch 5: 50000/50000\n",
      "Test: average loss: 0.6480, accuracy: 7750/10000 (78%)\n",
      "Train epoch 6: 50000/50000\n",
      "Test: average loss: 0.6812, accuracy: 7708/10000 (77%)\n",
      "Train epoch 7: 50000/50000\n",
      "Test: average loss: 0.6059, accuracy: 8042/10000 (80%)\n",
      "Train epoch 8: 50000/50000\n",
      "Test: average loss: 0.6395, accuracy: 7975/10000 (80%)\n",
      "Train epoch 9: 50000/50000\n",
      "Test: average loss: 0.6851, accuracy: 7933/10000 (79%)\n",
      "Train epoch 10: 50000/50000\n",
      "Test: average loss: 0.6507, accuracy: 7973/10000 (80%)\n"
     ]
    }
   ],
   "source": [
    "student_simple_model, student_simple_history = student_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(3,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            #2\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #3\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            #4\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #5\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #6\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #7\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #8\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #9\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #10\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #11\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #12\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #13\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.AvgPool2d(kernel_size=1,stride=1),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            #14\n",
    "            nn.Linear(512,4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #15\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #16\n",
    "            nn.Linear(4096,num_classes),\n",
    "            )\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        #        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        #        print(out.shape)\n",
    "        out = self.classifier(out)\n",
    "        #        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_teacher(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_main():\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=1000, shuffle=True)\n",
    "\n",
    "    model = VGG16().to(device)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters())\n",
    "    \n",
    "    teacher_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_teacher(model, device, train_loader, optimizer, epoch)\n",
    "        loss, acc = test_teacher(model, device, test_loader)\n",
    "        \n",
    "        teacher_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"teacher.pt\")\n",
    "    return model, teacher_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.8880, accuracy: 2846/10000 (28%)\n",
      "Train epoch 2: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.4881, accuracy: 3977/10000 (40%)\n",
      "Train epoch 3: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.2899, accuracy: 5523/10000 (55%)\n",
      "Train epoch 4: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.1057, accuracy: 6213/10000 (62%)\n",
      "Train epoch 5: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.9722, accuracy: 6767/10000 (68%)\n",
      "Train epoch 6: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.7948, accuracy: 7416/10000 (74%)\n",
      "Train epoch 7: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 1.0998, accuracy: 6904/10000 (69%)\n",
      "Train epoch 8: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6199, accuracy: 8013/10000 (80%)\n",
      "Train epoch 9: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6356, accuracy: 8038/10000 (80%)\n",
      "Train epoch 10: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5898, accuracy: 8117/10000 (81%)\n",
      "Train epoch 11: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5602, accuracy: 8228/10000 (82%)\n",
      "Train epoch 12: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5237, accuracy: 8417/10000 (84%)\n",
      "Train epoch 13: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6514, accuracy: 8006/10000 (80%)\n",
      "Train epoch 14: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5056, accuracy: 8434/10000 (84%)\n",
      "Train epoch 15: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4359, accuracy: 8636/10000 (86%)\n",
      "Train epoch 16: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5185, accuracy: 8465/10000 (85%)\n",
      "Train epoch 17: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4431, accuracy: 8614/10000 (86%)\n",
      "Train epoch 18: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5072, accuracy: 8619/10000 (86%)\n",
      "Train epoch 19: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4287, accuracy: 8717/10000 (87%)\n",
      "Train epoch 20: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4725, accuracy: 8705/10000 (87%)\n",
      "Train epoch 21: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5111, accuracy: 8598/10000 (86%)\n",
      "Train epoch 22: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4191, accuracy: 8810/10000 (88%)\n",
      "Train epoch 23: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5180, accuracy: 8678/10000 (87%)\n",
      "Train epoch 24: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5262, accuracy: 8713/10000 (87%)\n",
      "Train epoch 25: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5190, accuracy: 8670/10000 (87%)\n",
      "Train epoch 26: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.4627, accuracy: 8782/10000 (88%)\n",
      "Train epoch 27: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5431, accuracy: 8625/10000 (86%)\n",
      "Train epoch 28: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5029, accuracy: 8803/10000 (88%)\n",
      "Train epoch 29: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5131, accuracy: 8870/10000 (89%)\n",
      "Train epoch 30: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6168, accuracy: 8650/10000 (86%)\n",
      "Train epoch 31: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5187, accuracy: 8805/10000 (88%)\n",
      "Train epoch 32: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5482, accuracy: 8880/10000 (89%)\n",
      "Train epoch 33: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5194, accuracy: 8814/10000 (88%)\n",
      "Train epoch 34: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5560, accuracy: 8806/10000 (88%)\n",
      "Train epoch 35: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5127, accuracy: 8790/10000 (88%)\n",
      "Train epoch 36: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5708, accuracy: 8871/10000 (89%)\n",
      "Train epoch 37: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5854, accuracy: 8856/10000 (89%)\n",
      "Train epoch 38: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5084, accuracy: 8838/10000 (88%)\n",
      "Train epoch 39: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5684, accuracy: 8811/10000 (88%)\n",
      "Train epoch 40: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5758, accuracy: 8778/10000 (88%)\n",
      "Train epoch 41: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5993, accuracy: 8821/10000 (88%)\n",
      "Train epoch 42: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5629, accuracy: 8829/10000 (88%)\n",
      "Train epoch 43: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.7169, accuracy: 8812/10000 (88%)\n",
      "Train epoch 44: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.5987, accuracy: 8831/10000 (88%)\n",
      "Train epoch 45: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6302, accuracy: 8778/10000 (88%)\n",
      "Train epoch 46: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.7536, accuracy: 8743/10000 (87%)\n",
      "Train epoch 47: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6503, accuracy: 8865/10000 (89%)\n",
      "Train epoch 48: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6474, accuracy: 8692/10000 (87%)\n",
      "Train epoch 49: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6147, accuracy: 8860/10000 (89%)\n",
      "Train epoch 50: 50000/50000, [-------------------------------------------------->] 100%\n",
      "Test: average loss: 0.6426, accuracy: 8824/10000 (88%)\n"
     ]
    }
   ],
   "source": [
    "teacher_model, teacher_history = teacher_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def softmax_t(x, t):\n",
    "    x_exp = np.exp(x / t)\n",
    "    return x_exp / np.sum(x_exp)\n",
    "\n",
    "# test_loader_bs1 = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])),\n",
    "#     batch_size=1, shuffle=True)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "test_loader_bs1 = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teacher_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-116f0741d186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteacher_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader_bs1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teacher_model' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(test_loader_bs1))\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    output = teacher_model(data)\n",
    "\n",
    "test_x = data.cpu().numpy()\n",
    "y_out = output.cpu().numpy()\n",
    "y_out = y_out[0, ::]\n",
    "print('Output (NO softmax):', y_out)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(test_x[0, 0, ::])\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.bar(list(range(10)), softmax_t(y_out, 1), width=0.3)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.bar(list(range(10)), softmax_t(y_out, 10), width=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " teacherNet = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " teacherNet.load_state_dict(torch.load(\"./teacher.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacherNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacherNet.train(mode=False)\n",
    "teacherNet = teacherNet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (NO softmax): [-20.48947  -19.996756 -24.105139 -18.923903 -21.519308 -21.83338\n",
      " -20.308414 -19.067097 -18.018135  -6.518038]\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(test_loader_bs1))\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    output = teacherNet(data)\n",
    "\n",
    "test_x = data.cpu().numpy()\n",
    "y_out = output.cpu().numpy()\n",
    "y_out = y_out[0, ::]\n",
    "print('Output (NO softmax):', y_out)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(test_x[0, 0, ::])\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.bar(list(range(10)), softmax_t(y_out, 1), width=0.3)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.bar(list(range(10)), softmax_t(y_out, 10), width=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation(y, labels, teacher_scores, temp, alpha):\n",
    "    return nn.KLDivLoss()(F.log_softmax(y / temp, dim=1), F.softmax(teacher_scores / temp, dim=1)) * (\n",
    "            temp * temp * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_kd(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        teacher_output = teacherNet(data)\n",
    "        teacher_output = teacherNet.detach()  # 切断老师网络的反向传播，感谢B站“淡淡的落”的提醒\n",
    "        loss = distillation(output, target, teacher_output, temp=5.0, alpha=0.7)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student_kd(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
